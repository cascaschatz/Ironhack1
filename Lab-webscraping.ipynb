{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Web Scraping\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Web scraping can be defined as \"the construction of an agent to download, parse, and organize data from the web in an automated manner\". In this lab, you will practice a series of exercises to practice your web scraping skills.  \n",
    "\n",
    "Each exercise is independent from the previous one. If you get stuck in one exercise you can skip to the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints:\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "- Make sure you have all libraries installed before start the lab.  \n",
    "- In this lab you will use `requests`, `BeautifulSoup` and `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping github trending developers\n",
    "- In this first exercise we will scraping the github trending developers. Use the url below.\n",
    "```python\n",
    "url = 'https://github.com/trending/developers'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start using `requests.get()` over the 'url', save your output in a new variable called `get_html`\n",
    "- The output should be `<Response [200]>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_html = requests.get(url)\n",
    "get_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explore the request methods\n",
    "- Try get_html.status_code and get_html.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_html.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_html.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Call the `get_html.content` method to return the page content.\n",
    "- Save in a variable called `html_content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = get_html.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the BeautifulSoup to parse your result. You can use the code below.\n",
    "```python\n",
    "soup = BeautifulSoup(html_content, \"lxml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "- Find out the html tag and class names used for the developer names.\n",
    "- Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "- Use string manipulation techniques to replace whitespaces and line breaks (i.e. \\n) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['KentC.Dodds',\n",
    " 'SethVargo',\n",
    " 'VadimDemedes',\n",
    " 'PaulBeusterien',\n",
    " 'DanImhoff',\n",
    " 'CalebPorzio',\n",
    " 'TannerLinsley',\n",
    " 'InesMontani',\n",
    " 'Mr.doob',\n",
    " 'JacobHoffman-Andrews',\n",
    " 'TianonGravi',\n",
    " 'TaylorOtwell',\n",
    " 'MatthewJohnson',\n",
    " 'MathiasBuus',\n",
    " 'TimHolman',\n",
    " 'AlonZakai',\n",
    " 'HadleyWickham',\n",
    " 'Bo-YiWu',\n",
    " 'TobiasKoppers',\n",
    " 'KentaroWada',\n",
    " 'TeppeiFukuda',\n",
    " 'MartinAtkins',\n",
    " 'RyanMcKinley',\n",
    " 'KlausPost',\n",
    " 'JamesAgnew']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KlausPost',\n",
       " 'FlorianRoth',\n",
       " 'TomPayne',\n",
       " 'NielsLohmann',\n",
       " 'DomenicDenicola',\n",
       " 'JasonJohnston',\n",
       " 'ArielMashraki',\n",
       " 'RobertMosolgo',\n",
       " 'PySimpleGUI',\n",
       " 'TylerKing',\n",
       " 'RaphaÃ«lBenitte',\n",
       " 'FrancoisZaninotto',\n",
       " 'JeremyLong',\n",
       " 'RomanKhavronenko',\n",
       " 'JoeTsai',\n",
       " 'DotanSimha',\n",
       " 'ArvidNorberg',\n",
       " 'VincentDriessen',\n",
       " 'NathanRajlich',\n",
       " 'RobertZaremba',\n",
       " 'WillMcGugan',\n",
       " 'FernandGaliana',\n",
       " 'TaikiEndo',\n",
       " 'Jaco',\n",
       " 'KevinRing']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_list = [i.text.strip().replace(' ','').replace('\\n\\n', ' ') for i in soup.find_all('h1', {'class': 'h3 lh-condensed'})]\n",
    "dev_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping function\n",
    "- Now you have learned how to use Requests and BeautifulSoup. \n",
    "- Create the function below to make your scraping easier.\n",
    "```python\n",
    "def url_bs4(url):\n",
    "    get_html = requests.get(url)\n",
    "    print(get_html.status_code)\n",
    "    print(get_html.encoding)\n",
    "    html = get_html.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    return soup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_bs4(url):\n",
    "    get_html = requests.get(url)\n",
    "    print(get_html.status_code)\n",
    "    print(get_html.encoding)\n",
    "    html = get_html.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Walt Disney wikipedia page\n",
    "- Use the url below to scraping the Walt Disney Wikipedia page.\n",
    "- Use the url_bs4 function and check the status.\n",
    "```python\n",
    "url_disney = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "url_disney = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "soup_disney = url_bs4(url_disney)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a list with  all the image links from Walt Disney Wikipedia page\n",
    "- Try the `.find_all` method to find the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/128px-Walt_Disney_1942_signature.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [i.get('src').strip('//') for i in soup_disney.find_all('img')]\n",
    "links[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping earthquakes\n",
    "- Use the url below to scraping the 50 latest earthquakes.\n",
    "```python\n",
    "url_eq='https://www.emsc-csem.org/Earthquake/'\n",
    "```\n",
    "- Instead  of use requests and BeautifulSoup,  try the function `pd.read_html(url_eq)`\n",
    "- You will notice that it returns a list of elements. One of the elements in this list is the earthquake table\n",
    "- You will need to clean the columns names, the Date & Time values,  and drop the last 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_eq='https://www.emsc-csem.org/Earthquake/'\n",
    "earthquakes = pd.read_html(url_eq)\n",
    "earthquakes = earthquakes[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns names\n",
    "earthquakes.columns = [x[0].replace(' [+]','').replace(' [-]','') for x in earthquakes.columns]\n",
    "earthquakes = earthquakes.drop(columns=['CitizenResponse','Unnamed: 12_level_0','Mag'])\n",
    "earthquakes.columns = ['Date & Time UTC','Latitude', 'Latitude N-S','Longitude','Longitude E-W','Depth km','Mag','Region name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date & time values\n",
    "earthquakes['Date & Time UTC'] = earthquakes['Date & Time UTC'].apply(lambda x : str(x)[:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes = earthquakes.dropna(thresh=3).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes = earthquakes[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time UTC</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Latitude N-S</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Longitude E-W</th>\n",
       "      <th>Depth km</th>\n",
       "      <th>Mag</th>\n",
       "      <th>Region name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-03 00:27:36</td>\n",
       "      <td>35.45</td>\n",
       "      <td>N</td>\n",
       "      <td>3.55</td>\n",
       "      <td>W</td>\n",
       "      <td>16</td>\n",
       "      <td>2.7</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-09-03 00:23:51</td>\n",
       "      <td>19.20</td>\n",
       "      <td>N</td>\n",
       "      <td>155.42</td>\n",
       "      <td>W</td>\n",
       "      <td>33</td>\n",
       "      <td>2.3</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-09-03 00:07:20</td>\n",
       "      <td>19.52</td>\n",
       "      <td>N</td>\n",
       "      <td>155.65</td>\n",
       "      <td>W</td>\n",
       "      <td>5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-09-03 00:06:12</td>\n",
       "      <td>58.78</td>\n",
       "      <td>S</td>\n",
       "      <td>25.65</td>\n",
       "      <td>W</td>\n",
       "      <td>10</td>\n",
       "      <td>5.6</td>\n",
       "      <td>SOUTH SANDWICH ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-02 23:59:48</td>\n",
       "      <td>40.33</td>\n",
       "      <td>N</td>\n",
       "      <td>124.35</td>\n",
       "      <td>W</td>\n",
       "      <td>14</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date & Time UTC Latitude Latitude N-S Longitude Longitude E-W Depth km  \\\n",
       "0  2021-09-03 00:27:36    35.45            N      3.55             W       16   \n",
       "1  2021-09-03 00:23:51    19.20            N    155.42             W       33   \n",
       "2  2021-09-03 00:07:20    19.52            N    155.65             W        5   \n",
       "3  2021-09-03 00:06:12    58.78            S     25.65             W       10   \n",
       "4  2021-09-02 23:59:48    40.33            N    124.35             W       14   \n",
       "\n",
       "   Mag                    Region name  \n",
       "0  2.7            STRAIT OF GIBRALTAR  \n",
       "1  2.3       ISLAND OF HAWAII, HAWAII  \n",
       "2  2.6       ISLAND OF HAWAII, HAWAII  \n",
       "3  5.6  SOUTH SANDWICH ISLANDS REGION  \n",
       "4  2.2            NORTHERN CALIFORNIA  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- Find the IMDB's Top 250 data.\n",
    "- You should have movie name, year release, director name and actors.\n",
    "- Create a dataframe with the data you collected.\n",
    "- Use the url below to this exercise.\n",
    "```python\n",
    "url_imdb = 'https://www.imdb.com/chart/top'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Release</th>\n",
       "      <th>Director</th>\n",
       "      <th>Actors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Um Sonho de Liberdade</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins &amp;  Morgan Freeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O Poderoso ChefÃ£o</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando &amp;  Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O Poderoso ChefÃ£o II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Al Pacino &amp;  Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Batman: O Cavaleiro das Trevas</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale &amp;  Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Homens e uma SentenÃ§a</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Henry Fonda &amp;  Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Rififi</td>\n",
       "      <td>1955</td>\n",
       "      <td>Jules Dassin</td>\n",
       "      <td>Jean Servais &amp;  Carl MÃ¶hner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Aurora</td>\n",
       "      <td>1927</td>\n",
       "      <td>F.W. Murnau</td>\n",
       "      <td>George O'Brien &amp;  Janet Gaynor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Noites de CabÃ­ria</td>\n",
       "      <td>1957</td>\n",
       "      <td>Federico Fellini</td>\n",
       "      <td>Giulietta Masina &amp;  FranÃ§ois PÃ©rier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Ratsasan</td>\n",
       "      <td>2018</td>\n",
       "      <td>Ram Kumar</td>\n",
       "      <td>Vishnu Vishal &amp;  Amala Paul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Tumbbad</td>\n",
       "      <td>2018</td>\n",
       "      <td>Rahi Anil Barve</td>\n",
       "      <td>Sohum Shah &amp;  Jyoti Malshe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Title Release              Director  \\\n",
       "0             Um Sonho de Liberdade    1994        Frank Darabont   \n",
       "1                 O Poderoso ChefÃ£o    1972  Francis Ford Coppola   \n",
       "2              O Poderoso ChefÃ£o II    1974  Francis Ford Coppola   \n",
       "3    Batman: O Cavaleiro das Trevas    2008     Christopher Nolan   \n",
       "4          12 Homens e uma SentenÃ§a    1957          Sidney Lumet   \n",
       "..                              ...     ...                   ...   \n",
       "245                          Rififi    1955          Jules Dassin   \n",
       "246                          Aurora    1927           F.W. Murnau   \n",
       "247               Noites de CabÃ­ria    1957      Federico Fellini   \n",
       "248                        Ratsasan    2018             Ram Kumar   \n",
       "249                         Tumbbad    2018       Rahi Anil Barve   \n",
       "\n",
       "                                   Actors  \n",
       "0           Tim Robbins &  Morgan Freeman  \n",
       "1              Marlon Brando &  Al Pacino  \n",
       "2             Al Pacino &  Robert De Niro  \n",
       "3          Christian Bale &  Heath Ledger  \n",
       "4              Henry Fonda &  Lee J. Cobb  \n",
       "..                                    ...  \n",
       "245           Jean Servais &  Carl MÃ¶hner  \n",
       "246        George O'Brien &  Janet Gaynor  \n",
       "247   Giulietta Masina &  FranÃ§ois PÃ©rier  \n",
       "248           Vishnu Vishal &  Amala Paul  \n",
       "249            Sohum Shah &  Jyoti Malshe  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_imdb = 'https://www.imdb.com/chart/top'\n",
    "html = requests.get(url_imdb).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "\n",
    "movies = soup.find_all('td', {'class':'titleColumn'})\n",
    "titles = [movie.find('a').text for movie in movies]\n",
    "years = [movie.find('span').text[1:-1] for movie in movies]\n",
    "directors = [movie.find('a').get('title').split(',')[0][:-7] for movie in movies]\n",
    "actors = [' & '.join(movie.find('a').get('title').split(',')[1:]) for movie in movies]\n",
    "\n",
    "movies_dict = {'Title': titles, 'Release': years, 'Director': directors, 'Actors': actors}\n",
    "\n",
    "movies_df = pd.DataFrame(movies_dict)\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
